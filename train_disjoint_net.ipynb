{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disjoint-domain network\n",
    "\n",
    "### Ethan Blackwood\n",
    "### October 23, 2020\n",
    "\n",
    "**Goal**: Train and analyze the network in Rogers/McClelland 2008 with 4 disjoint domains (Figures R3-R5), which learns to extract the feature of being more or less similar to other items in the same domain, across the 4 domains which have no items, contexts or attributes in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import disjoint_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get inputs and outputs with particular similarity structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can afford to use doubles for this\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "n_domains = 4\n",
    "n_contexts = 4\n",
    "attrs_per_context = 50\n",
    "\n",
    "domains = [chr(ord('A') + d) for d in range(n_domains)]\n",
    "item_mat, context_mat, attr_mat = disjoint_domain.make_io_mats(\n",
    "    n_contexts=n_contexts, attrs_per_context=attrs_per_context, n_domains=n_domains, rng=rng)\n",
    "\n",
    "x_item = torch.tensor(item_mat, dtype=torch.double)\n",
    "x_context = torch.tensor(context_mat, dtype=torch.double)\n",
    "y = torch.tensor(attr_mat, dtype=torch.double)\n",
    "\n",
    "# Make some variables for individual inputs for convenience later\n",
    "items = np.eye(disjoint_domain.N_ITEMS * n_domains, dtype=np.float64)\n",
    "def itemgroup(n):\n",
    "    if n < 4:\n",
    "        return '()'\n",
    "    elif n < 6:\n",
    "        return '[]'\n",
    "    else:\n",
    "        return '{}'\n",
    "\n",
    "item_names = [d + str(n+1) + itemgroup(n) for d in domains for n in range(disjoint_domain.N_ITEMS)]\n",
    "\n",
    "contexts = np.eye(n_contexts * n_domains, dtype=np.float64)\n",
    "context_names = [d + str(n+1) for d in domains for n in range(n_contexts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the network and training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisjointDomainNet(nn.Module):\n",
    "    def __init__(self, n_contexts, attrs_per_context, n_domains):\n",
    "        super(DisjointDomainNet, self).__init__()\n",
    "        \n",
    "        self.n_items = disjoint_domain.N_ITEMS * n_domains\n",
    "        self.n_contexts = n_contexts * n_domains\n",
    "        self.n_attributes = attrs_per_context * self.n_contexts \n",
    "        \n",
    "        # Not sure if these are reasonable\n",
    "        item_rep_size = self.n_items\n",
    "        ctx_rep_size = self.n_contexts\n",
    "        hidden_size = item_rep_size * 2\n",
    "        \n",
    "        # define layers\n",
    "        self.item_to_irep = nn.Linear(self.n_items, item_rep_size)\n",
    "        self.ctx_to_crep = nn.Linear(self.n_contexts, ctx_rep_size)\n",
    "        self.irep_to_hidden = nn.Linear(item_rep_size, hidden_size)\n",
    "        self.crep_to_hidden = nn.Linear(ctx_rep_size, hidden_size, bias=False) # only need 1 hidden layer bias\n",
    "        self.hidden_to_attr = nn.Linear(hidden_size, self.n_attributes)\n",
    "        \n",
    "        # make weights start small\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters():\n",
    "                nn.init.normal_(p.data, std=0.01)\n",
    "                #nn.init.uniform_(p.data, a=-0.01, b=0.01)\n",
    "                \n",
    "    def forward(self, item, context):\n",
    "        irep = torch.sigmoid(self.item_to_irep(item))\n",
    "        crep = torch.sigmoid(self.ctx_to_crep(context))\n",
    "        hidden = torch.sigmoid(self.irep_to_hidden(irep) + self.crep_to_hidden(crep))\n",
    "        attr = torch.sigmoid(self.hidden_to_attr(hidden))\n",
    "        return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, optimizer, num_epochs=200, snap_freq=20, batch_size=4, scheduler=None):\n",
    "    \n",
    "    n_snaps = num_epochs // snap_freq\n",
    "    n_items = net.n_items\n",
    "    n_rep = net.item_to_irep.out_features\n",
    "    \n",
    "    # Holds snapshots of input representation layer after probing with each item\n",
    "    rep_snapshots = np.ndarray((n_snaps, n_items, n_rep))\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    n_items = len(y)\n",
    "    n_batches = (n_items-1) // batch_size + 1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # collect snapshot\n",
    "        if epoch % snap_freq == 0:\n",
    "            k_snap = epoch // snap_freq\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for k_item, item in enumerate(items):\n",
    "                    act = torch.sigmoid(net.item_to_irep(torch.tensor(item)))\n",
    "                    rep_snapshots[k_snap, k_item, :] = act\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        order = rng.permutation(n_items)\n",
    "        for k_batch in range(n_batches):\n",
    "            # train\n",
    "            batch_inds = order[k_batch*batch_size:(k_batch+1)*batch_size] \n",
    "            \n",
    "            outputs = net(x_item[batch_inds], x_context[batch_inds])\n",
    "            loss = criterion(outputs, y[batch_inds])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item() * len(batch_inds)\n",
    "                accuracy = torch.mean(((outputs > 0.5).to(torch.double) == y[batch_inds]).to(torch.double))\n",
    "                running_accuracy += accuracy.item() * len(batch_inds)\n",
    "        \n",
    "        if epoch % snap_freq == 0:\n",
    "            print(f'Epoch {epoch} end: mean loss = {running_loss / n_items:.3f}, mean accuracy = {running_accuracy / n_items:.3f}')\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "    return rep_snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment of truth, time to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 end: mean loss = 0.241, mean accuracy = 0.771\n",
      "Epoch 1000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 2000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 3000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 4000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 5000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 6000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 7000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 8000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 9000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 10000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 11000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 12000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 13000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 14000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 15000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 16000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 17000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 18000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 19000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 20000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 21000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 22000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 23000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 24000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 25000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 26000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 27000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 28000 end: mean loss = 0.030, mean accuracy = 0.969\n",
      "Epoch 29000 end: mean loss = 0.030, mean accuracy = 0.969\n"
     ]
    }
   ],
   "source": [
    "net = DisjointDomainNet(n_contexts, attrs_per_context, n_domains)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.95)\n",
    "\n",
    "rep_snapshots = train_network(net, optimizer, batch_size=4, snap_freq=1000, num_epochs=30000, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42c92a03e814f19b54e21436a06e9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = hierarchy.linkage(rep_snapshots[-1])\n",
    "plt.figure()\n",
    "hierarchy.dendrogram(z, labels=item_names)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
