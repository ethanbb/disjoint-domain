{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Rumelhart and Todd network (1993)\n",
    "\n",
    "### Ethan Blackwood\n",
    "### September 28, 2020\n",
    "\n",
    "**Goal**: Simulate the Rumelhart & Todd connectionist semantic memory network shown in Rogers & McClelland (2008)\n",
    "Figure 1, and replicate the results in Figure 3 regarding the similarity of internal item representations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "import ptree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, build the tree that contains all our inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items:  ['rose', 'pine', 'sunfish', 'oak', 'canary', 'salmon', 'robin', 'daisy']\n",
      "Relations:  ['is', 'has', 'can', 'ISA']\n",
      "Attributes:  ['scales', 'skin', 'oak', 'fly', 'bark', 'salmon', 'sing', 'organism', 'living', 'red', 'move', 'rose', 'grow', 'pine', 'branches', 'green', 'pretty', 'bird', 'leaves', 'yellow', 'animal', 'flower', 'big', 'petals', 'plant', 'sunfish', 'swim', 'canary', 'feathers', 'gills', 'robin', 'roots', 'tree', 'fish', 'wings', 'daisy']\n",
      "\n",
      "Some examples:\n",
      "robin is: living, red\n",
      "oak has: bark, branches, leaves, roots\n",
      "sunfish has: scales, skin, gills\n",
      "canary is: living, yellow\n"
     ]
    }
   ],
   "source": [
    "# can afford to use doubles for this\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "rumeltree = ptree.from_xml('rumeltree.xml')\n",
    "\n",
    "# Convert to lists so we have a canonical order for items, relations, and attributes.\n",
    "items = list(rumeltree['items'])\n",
    "relations = list(rumeltree['relations'])\n",
    "attributes = list(rumeltree['attributes'])\n",
    "\n",
    "# Now make our inputs and outputs.\n",
    "item_vecs = torch.eye(len(items)).split(1)\n",
    "rel_vecs = torch.eye(len(relations)).split(1)\n",
    "xs = list(itertools.product(item_vecs, rel_vecs))\n",
    "items_rep = torch.stack([x[0] for x in xs], axis=0)\n",
    "rels_rep = torch.stack([x[1] for x in xs], axis=0)\n",
    "xs_cat = (items_rep, rels_rep)\n",
    "\n",
    "y = torch.zeros((len(xs), len(attributes)))\n",
    "\n",
    "for kI in range(len(items)):\n",
    "    for kR in range(len(relations)):\n",
    "\n",
    "        # get attributes to associate\n",
    "        my_attrs = rumeltree['nodes'][items[kI]].get_related_attributes(relations[kR])\n",
    "        attr_inds = np.isin(attributes, list(my_attrs))\n",
    "        y[kI*len(relations) + kR, attr_inds] = 1\n",
    "\n",
    "ys = y.split(1)\n",
    "ys_cat = y\n",
    "\n",
    "# prepare for MultiLabelMarginLoss\n",
    "# y_inds = torch.full((len(ys), len(attributes)), -1, dtype=torch.long)\n",
    "# for i, y_vec in enumerate(ys):\n",
    "#    y_ind = y_vec.nonzero(as_tuple=True)[1]\n",
    "#    y_inds[i, :len(y_ind)] = y_ind\n",
    "\n",
    "# y_inds = y_inds.split(1)\n",
    "\n",
    "print('Items: ', items)\n",
    "print('Relations: ', relations)\n",
    "print('Attributes: ', attributes)\n",
    "print()\n",
    "print('Some examples:')\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "for k in rng.choice(len(xs), size=4, replace=False):\n",
    "    x = xs[k]\n",
    "    item_hot = x[0].numpy().squeeze().nonzero()[0]\n",
    "    item = items[item_hot[0]]\n",
    "    rel_hot = x[1].numpy().squeeze().nonzero()[0]\n",
    "    relation = relations[rel_hot[0]]\n",
    "    \n",
    "    y = ys[k]\n",
    "    attrs_hot = y.numpy().squeeze().nonzero()[0]\n",
    "    attrs = [attributes[i] for i in attrs_hot]\n",
    "    \n",
    "    print(f'{item} {relation}: {\", \".join(attrs) if len(attrs) > 0 else \"<nothing>\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the network and training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumelNet(nn.Module):\n",
    "    def __init__(self, n_items, n_relations, n_attributes):\n",
    "        super(RumelNet, self).__init__()\n",
    "        \n",
    "        self.n_items = n_items\n",
    "        self.n_relations = n_relations\n",
    "        self.n_attributes = n_attributes\n",
    "        \n",
    "        rep_size = 8\n",
    "        hidden_size = 15\n",
    "        \n",
    "        # define layers\n",
    "        self.item_to_rep = nn.Linear(n_items, rep_size)\n",
    "        self.rep_to_hidden = nn.Linear(rep_size, hidden_size)\n",
    "        self.rel_to_hidden = nn.Linear(n_relations, hidden_size, bias=False) # only need one hidden layer bias\n",
    "        self.hidden_to_attr = nn.Linear(hidden_size, n_attributes)\n",
    "        \n",
    "        # make weights start small\n",
    "        with torch.no_grad():\n",
    "            for layer in (self.item_to_rep, self.rep_to_hidden, self.rel_to_hidden, self.hidden_to_attr):\n",
    "                nn.init.normal_(layer.weight.data, std=0.1)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.normal_(layer.bias.data, std=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # split into item and relation\n",
    "        item, relation = x\n",
    "        \n",
    "        # flow inputs through network\n",
    "        rep = torch.sigmoid(self.item_to_rep(item))\n",
    "        hidden = torch.sigmoid(self.rep_to_hidden(rep) + self.rel_to_hidden(relation))\n",
    "        attr = torch.sigmoid(self.hidden_to_attr(hidden))\n",
    "        return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, optimizer, num_epochs=200, snap_freq=20, batch_size=4):\n",
    "    \n",
    "    n_snaps = num_epochs // snap_freq\n",
    "    n_items = net.n_items\n",
    "    n_rep = net.item_to_rep.out_features\n",
    "    \n",
    "    # Holds snapshots of input representation layer after probing with each item\n",
    "    rep_snapshots = np.ndarray((n_snaps, n_items, n_rep))\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    n_batches = (len(xs)-1) // batch_size + 1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # collect snapshot\n",
    "        if epoch % snap_freq == 0:\n",
    "            k_snap = epoch // snap_freq\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for k_item, item in enumerate(item_vecs):\n",
    "                    act = torch.sigmoid(net.item_to_rep(item))\n",
    "                    rep_snapshots[k_snap, k_item, :] = act\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = net(xs_cat)\n",
    "#         loss = criterion(outputs, ys_cat)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if epoch % 10 == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 running_loss = loss.item()\n",
    "#                 running_accuracy = torch.mean(((outputs > 0.5).to(torch.double) == ys_cat).to(torch.double)).item()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        order = rng.permutation(len(xs))\n",
    "        for k_batch in range(n_batches):\n",
    "            # train\n",
    "            optimizer.zero_grad()\n",
    "            batch_inds = order[k_batch*batch_size:(k_batch+1)*batch_size] \n",
    "            \n",
    "            outputs = net((xs_cat[0][batch_inds], xs_cat[1][batch_inds]))\n",
    "            loss = criterion(outputs, ys_cat[batch_inds])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item() * len(batch_inds)\n",
    "                accuracy = torch.mean(((outputs > 0.5).to(torch.double) == ys_cat[batch_inds]).to(torch.double))\n",
    "                running_accuracy += accuracy.item() * len(batch_inds)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch} end: mean loss = {running_loss / len(xs):.3f}, mean accuracy = {running_accuracy / len(xs):.3f}')\n",
    "        \n",
    "    return rep_snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment of truth, time to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 end: mean loss = 0.254, mean accuracy = 0.489\n",
      "Epoch 10 end: mean loss = 0.254, mean accuracy = 0.489\n",
      "Epoch 20 end: mean loss = 0.253, mean accuracy = 0.489\n",
      "Epoch 30 end: mean loss = 0.253, mean accuracy = 0.492\n",
      "Epoch 40 end: mean loss = 0.252, mean accuracy = 0.495\n",
      "Epoch 50 end: mean loss = 0.252, mean accuracy = 0.495\n",
      "Epoch 60 end: mean loss = 0.251, mean accuracy = 0.502\n",
      "Epoch 70 end: mean loss = 0.251, mean accuracy = 0.507\n",
      "Epoch 80 end: mean loss = 0.250, mean accuracy = 0.507\n",
      "Epoch 90 end: mean loss = 0.250, mean accuracy = 0.511\n",
      "Epoch 100 end: mean loss = 0.250, mean accuracy = 0.513\n",
      "Epoch 110 end: mean loss = 0.249, mean accuracy = 0.513\n",
      "Epoch 120 end: mean loss = 0.249, mean accuracy = 0.513\n",
      "Epoch 130 end: mean loss = 0.248, mean accuracy = 0.513\n",
      "Epoch 140 end: mean loss = 0.248, mean accuracy = 0.518\n",
      "Epoch 150 end: mean loss = 0.247, mean accuracy = 0.526\n",
      "Epoch 160 end: mean loss = 0.247, mean accuracy = 0.533\n",
      "Epoch 170 end: mean loss = 0.246, mean accuracy = 0.533\n",
      "Epoch 180 end: mean loss = 0.246, mean accuracy = 0.538\n",
      "Epoch 190 end: mean loss = 0.245, mean accuracy = 0.537\n",
      "Epoch 200 end: mean loss = 0.245, mean accuracy = 0.546\n",
      "Epoch 210 end: mean loss = 0.244, mean accuracy = 0.550\n",
      "Epoch 220 end: mean loss = 0.244, mean accuracy = 0.557\n",
      "Epoch 230 end: mean loss = 0.243, mean accuracy = 0.556\n",
      "Epoch 240 end: mean loss = 0.243, mean accuracy = 0.561\n",
      "Epoch 250 end: mean loss = 0.242, mean accuracy = 0.563\n",
      "Epoch 260 end: mean loss = 0.242, mean accuracy = 0.563\n",
      "Epoch 270 end: mean loss = 0.242, mean accuracy = 0.563\n",
      "Epoch 280 end: mean loss = 0.241, mean accuracy = 0.563\n",
      "Epoch 290 end: mean loss = 0.241, mean accuracy = 0.563\n",
      "Epoch 300 end: mean loss = 0.240, mean accuracy = 0.563\n",
      "Epoch 310 end: mean loss = 0.240, mean accuracy = 0.563\n",
      "Epoch 320 end: mean loss = 0.239, mean accuracy = 0.567\n",
      "Epoch 330 end: mean loss = 0.239, mean accuracy = 0.572\n",
      "Epoch 340 end: mean loss = 0.238, mean accuracy = 0.575\n",
      "Epoch 350 end: mean loss = 0.238, mean accuracy = 0.576\n",
      "Epoch 360 end: mean loss = 0.237, mean accuracy = 0.576\n",
      "Epoch 370 end: mean loss = 0.237, mean accuracy = 0.576\n",
      "Epoch 380 end: mean loss = 0.237, mean accuracy = 0.576\n",
      "Epoch 390 end: mean loss = 0.236, mean accuracy = 0.582\n",
      "Epoch 400 end: mean loss = 0.236, mean accuracy = 0.590\n",
      "Epoch 410 end: mean loss = 0.235, mean accuracy = 0.605\n",
      "Epoch 420 end: mean loss = 0.235, mean accuracy = 0.633\n",
      "Epoch 430 end: mean loss = 0.234, mean accuracy = 0.641\n",
      "Epoch 440 end: mean loss = 0.234, mean accuracy = 0.644\n",
      "Epoch 450 end: mean loss = 0.233, mean accuracy = 0.645\n",
      "Epoch 460 end: mean loss = 0.233, mean accuracy = 0.648\n",
      "Epoch 470 end: mean loss = 0.233, mean accuracy = 0.657\n",
      "Epoch 480 end: mean loss = 0.232, mean accuracy = 0.657\n",
      "Epoch 490 end: mean loss = 0.232, mean accuracy = 0.657\n",
      "Epoch 500 end: mean loss = 0.231, mean accuracy = 0.659\n",
      "Epoch 510 end: mean loss = 0.231, mean accuracy = 0.668\n",
      "Epoch 520 end: mean loss = 0.230, mean accuracy = 0.672\n",
      "Epoch 530 end: mean loss = 0.230, mean accuracy = 0.674\n",
      "Epoch 540 end: mean loss = 0.229, mean accuracy = 0.674\n",
      "Epoch 550 end: mean loss = 0.229, mean accuracy = 0.674\n",
      "Epoch 560 end: mean loss = 0.229, mean accuracy = 0.674\n",
      "Epoch 570 end: mean loss = 0.228, mean accuracy = 0.674\n",
      "Epoch 580 end: mean loss = 0.228, mean accuracy = 0.674\n",
      "Epoch 590 end: mean loss = 0.227, mean accuracy = 0.674\n",
      "Epoch 600 end: mean loss = 0.227, mean accuracy = 0.678\n",
      "Epoch 610 end: mean loss = 0.227, mean accuracy = 0.681\n",
      "Epoch 620 end: mean loss = 0.226, mean accuracy = 0.681\n",
      "Epoch 630 end: mean loss = 0.226, mean accuracy = 0.681\n",
      "Epoch 640 end: mean loss = 0.225, mean accuracy = 0.688\n",
      "Epoch 650 end: mean loss = 0.225, mean accuracy = 0.688\n",
      "Epoch 660 end: mean loss = 0.224, mean accuracy = 0.690\n",
      "Epoch 670 end: mean loss = 0.224, mean accuracy = 0.697\n",
      "Epoch 680 end: mean loss = 0.224, mean accuracy = 0.704\n",
      "Epoch 690 end: mean loss = 0.223, mean accuracy = 0.711\n",
      "Epoch 700 end: mean loss = 0.223, mean accuracy = 0.721\n",
      "Epoch 710 end: mean loss = 0.222, mean accuracy = 0.728\n",
      "Epoch 720 end: mean loss = 0.222, mean accuracy = 0.728\n",
      "Epoch 730 end: mean loss = 0.222, mean accuracy = 0.734\n",
      "Epoch 740 end: mean loss = 0.221, mean accuracy = 0.734\n",
      "Epoch 750 end: mean loss = 0.221, mean accuracy = 0.735\n",
      "Epoch 760 end: mean loss = 0.220, mean accuracy = 0.736\n",
      "Epoch 770 end: mean loss = 0.220, mean accuracy = 0.747\n",
      "Epoch 780 end: mean loss = 0.219, mean accuracy = 0.747\n",
      "Epoch 790 end: mean loss = 0.219, mean accuracy = 0.747\n",
      "Epoch 800 end: mean loss = 0.219, mean accuracy = 0.747\n",
      "Epoch 810 end: mean loss = 0.218, mean accuracy = 0.747\n",
      "Epoch 820 end: mean loss = 0.218, mean accuracy = 0.747\n",
      "Epoch 830 end: mean loss = 0.217, mean accuracy = 0.747\n",
      "Epoch 840 end: mean loss = 0.217, mean accuracy = 0.747\n",
      "Epoch 850 end: mean loss = 0.217, mean accuracy = 0.747\n",
      "Epoch 860 end: mean loss = 0.216, mean accuracy = 0.747\n",
      "Epoch 870 end: mean loss = 0.216, mean accuracy = 0.747\n",
      "Epoch 880 end: mean loss = 0.215, mean accuracy = 0.747\n",
      "Epoch 890 end: mean loss = 0.215, mean accuracy = 0.747\n",
      "Epoch 900 end: mean loss = 0.215, mean accuracy = 0.750\n",
      "Epoch 910 end: mean loss = 0.214, mean accuracy = 0.754\n",
      "Epoch 920 end: mean loss = 0.214, mean accuracy = 0.754\n",
      "Epoch 930 end: mean loss = 0.214, mean accuracy = 0.754\n",
      "Epoch 940 end: mean loss = 0.213, mean accuracy = 0.754\n",
      "Epoch 950 end: mean loss = 0.213, mean accuracy = 0.760\n",
      "Epoch 960 end: mean loss = 0.212, mean accuracy = 0.760\n",
      "Epoch 970 end: mean loss = 0.212, mean accuracy = 0.773\n",
      "Epoch 980 end: mean loss = 0.212, mean accuracy = 0.786\n",
      "Epoch 990 end: mean loss = 0.211, mean accuracy = 0.793\n",
      "Epoch 1000 end: mean loss = 0.211, mean accuracy = 0.794\n",
      "Epoch 1010 end: mean loss = 0.210, mean accuracy = 0.794\n",
      "Epoch 1020 end: mean loss = 0.210, mean accuracy = 0.794\n",
      "Epoch 1030 end: mean loss = 0.210, mean accuracy = 0.794\n",
      "Epoch 1040 end: mean loss = 0.209, mean accuracy = 0.794\n",
      "Epoch 1050 end: mean loss = 0.209, mean accuracy = 0.794\n",
      "Epoch 1060 end: mean loss = 0.209, mean accuracy = 0.794\n",
      "Epoch 1070 end: mean loss = 0.208, mean accuracy = 0.794\n",
      "Epoch 1080 end: mean loss = 0.208, mean accuracy = 0.794\n",
      "Epoch 1090 end: mean loss = 0.207, mean accuracy = 0.794\n",
      "Epoch 1100 end: mean loss = 0.207, mean accuracy = 0.794\n",
      "Epoch 1110 end: mean loss = 0.207, mean accuracy = 0.794\n",
      "Epoch 1120 end: mean loss = 0.206, mean accuracy = 0.794\n",
      "Epoch 1130 end: mean loss = 0.206, mean accuracy = 0.794\n",
      "Epoch 1140 end: mean loss = 0.206, mean accuracy = 0.794\n",
      "Epoch 1150 end: mean loss = 0.205, mean accuracy = 0.794\n",
      "Epoch 1160 end: mean loss = 0.205, mean accuracy = 0.794\n",
      "Epoch 1170 end: mean loss = 0.205, mean accuracy = 0.794\n",
      "Epoch 1180 end: mean loss = 0.204, mean accuracy = 0.794\n",
      "Epoch 1190 end: mean loss = 0.204, mean accuracy = 0.794\n",
      "Epoch 1200 end: mean loss = 0.203, mean accuracy = 0.794\n",
      "Epoch 1210 end: mean loss = 0.203, mean accuracy = 0.794\n",
      "Epoch 1220 end: mean loss = 0.203, mean accuracy = 0.794\n",
      "Epoch 1230 end: mean loss = 0.202, mean accuracy = 0.794\n",
      "Epoch 1240 end: mean loss = 0.202, mean accuracy = 0.794\n",
      "Epoch 1250 end: mean loss = 0.202, mean accuracy = 0.794\n",
      "Epoch 1260 end: mean loss = 0.201, mean accuracy = 0.794\n",
      "Epoch 1270 end: mean loss = 0.201, mean accuracy = 0.794\n",
      "Epoch 1280 end: mean loss = 0.201, mean accuracy = 0.794\n",
      "Epoch 1290 end: mean loss = 0.200, mean accuracy = 0.794\n",
      "Epoch 1300 end: mean loss = 0.200, mean accuracy = 0.794\n",
      "Epoch 1310 end: mean loss = 0.200, mean accuracy = 0.794\n",
      "Epoch 1320 end: mean loss = 0.199, mean accuracy = 0.794\n",
      "Epoch 1330 end: mean loss = 0.199, mean accuracy = 0.794\n",
      "Epoch 1340 end: mean loss = 0.199, mean accuracy = 0.794\n",
      "Epoch 1350 end: mean loss = 0.198, mean accuracy = 0.794\n",
      "Epoch 1360 end: mean loss = 0.198, mean accuracy = 0.794\n",
      "Epoch 1370 end: mean loss = 0.197, mean accuracy = 0.794\n",
      "Epoch 1380 end: mean loss = 0.197, mean accuracy = 0.794\n",
      "Epoch 1390 end: mean loss = 0.197, mean accuracy = 0.794\n",
      "Epoch 1400 end: mean loss = 0.196, mean accuracy = 0.794\n",
      "Epoch 1410 end: mean loss = 0.196, mean accuracy = 0.794\n",
      "Epoch 1420 end: mean loss = 0.196, mean accuracy = 0.799\n",
      "Epoch 1430 end: mean loss = 0.195, mean accuracy = 0.809\n",
      "Epoch 1440 end: mean loss = 0.195, mean accuracy = 0.811\n",
      "Epoch 1450 end: mean loss = 0.195, mean accuracy = 0.810\n",
      "Epoch 1460 end: mean loss = 0.194, mean accuracy = 0.819\n",
      "Epoch 1470 end: mean loss = 0.194, mean accuracy = 0.818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1480 end: mean loss = 0.194, mean accuracy = 0.823\n",
      "Epoch 1490 end: mean loss = 0.193, mean accuracy = 0.826\n",
      "Epoch 1500 end: mean loss = 0.193, mean accuracy = 0.827\n",
      "Epoch 1510 end: mean loss = 0.193, mean accuracy = 0.829\n",
      "Epoch 1520 end: mean loss = 0.192, mean accuracy = 0.828\n",
      "Epoch 1530 end: mean loss = 0.192, mean accuracy = 0.828\n",
      "Epoch 1540 end: mean loss = 0.192, mean accuracy = 0.828\n",
      "Epoch 1550 end: mean loss = 0.191, mean accuracy = 0.834\n",
      "Epoch 1560 end: mean loss = 0.191, mean accuracy = 0.837\n",
      "Epoch 1570 end: mean loss = 0.191, mean accuracy = 0.843\n",
      "Epoch 1580 end: mean loss = 0.190, mean accuracy = 0.845\n",
      "Epoch 1590 end: mean loss = 0.190, mean accuracy = 0.845\n",
      "Epoch 1600 end: mean loss = 0.190, mean accuracy = 0.846\n",
      "Epoch 1610 end: mean loss = 0.189, mean accuracy = 0.845\n",
      "Epoch 1620 end: mean loss = 0.189, mean accuracy = 0.846\n",
      "Epoch 1630 end: mean loss = 0.189, mean accuracy = 0.845\n",
      "Epoch 1640 end: mean loss = 0.189, mean accuracy = 0.846\n",
      "Epoch 1650 end: mean loss = 0.188, mean accuracy = 0.846\n",
      "Epoch 1660 end: mean loss = 0.188, mean accuracy = 0.855\n",
      "Epoch 1670 end: mean loss = 0.188, mean accuracy = 0.856\n",
      "Epoch 1680 end: mean loss = 0.187, mean accuracy = 0.862\n",
      "Epoch 1690 end: mean loss = 0.187, mean accuracy = 0.869\n",
      "Epoch 1700 end: mean loss = 0.187, mean accuracy = 0.869\n",
      "Epoch 1710 end: mean loss = 0.186, mean accuracy = 0.873\n",
      "Epoch 1720 end: mean loss = 0.186, mean accuracy = 0.875\n",
      "Epoch 1730 end: mean loss = 0.186, mean accuracy = 0.878\n",
      "Epoch 1740 end: mean loss = 0.185, mean accuracy = 0.882\n",
      "Epoch 1750 end: mean loss = 0.185, mean accuracy = 0.885\n",
      "Epoch 1760 end: mean loss = 0.185, mean accuracy = 0.886\n",
      "Epoch 1770 end: mean loss = 0.184, mean accuracy = 0.894\n",
      "Epoch 1780 end: mean loss = 0.184, mean accuracy = 0.905\n",
      "Epoch 1790 end: mean loss = 0.184, mean accuracy = 0.910\n",
      "Epoch 1800 end: mean loss = 0.184, mean accuracy = 0.914\n",
      "Epoch 1810 end: mean loss = 0.183, mean accuracy = 0.919\n",
      "Epoch 1820 end: mean loss = 0.183, mean accuracy = 0.919\n",
      "Epoch 1830 end: mean loss = 0.183, mean accuracy = 0.919\n",
      "Epoch 1840 end: mean loss = 0.182, mean accuracy = 0.919\n",
      "Epoch 1850 end: mean loss = 0.182, mean accuracy = 0.919\n",
      "Epoch 1860 end: mean loss = 0.182, mean accuracy = 0.919\n",
      "Epoch 1870 end: mean loss = 0.181, mean accuracy = 0.919\n",
      "Epoch 1880 end: mean loss = 0.181, mean accuracy = 0.919\n",
      "Epoch 1890 end: mean loss = 0.181, mean accuracy = 0.919\n",
      "Epoch 1900 end: mean loss = 0.180, mean accuracy = 0.919\n",
      "Epoch 1910 end: mean loss = 0.180, mean accuracy = 0.919\n",
      "Epoch 1920 end: mean loss = 0.180, mean accuracy = 0.919\n",
      "Epoch 1930 end: mean loss = 0.180, mean accuracy = 0.919\n",
      "Epoch 1940 end: mean loss = 0.179, mean accuracy = 0.919\n",
      "Epoch 1950 end: mean loss = 0.179, mean accuracy = 0.919\n",
      "Epoch 1960 end: mean loss = 0.179, mean accuracy = 0.919\n",
      "Epoch 1970 end: mean loss = 0.178, mean accuracy = 0.919\n",
      "Epoch 1980 end: mean loss = 0.178, mean accuracy = 0.919\n",
      "Epoch 1990 end: mean loss = 0.178, mean accuracy = 0.919\n"
     ]
    }
   ],
   "source": [
    "net = RumelNet(len(items), len(relations), len(attributes))\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "rep_snapshots = train_network(net, optimizer, num_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e80ba7cd8ff40309741318648dd46e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = hierarchy.linkage(rep_snapshots[-1])\n",
    "plt.figure()\n",
    "hierarchy.dendrogram(z, labels=items)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d438e8dcc5428e8f21da1129d5a131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test trained net\n",
    "with torch.no_grad():\n",
    "    ind = 31\n",
    "    \n",
    "    item_vec, rel_vec = [t.squeeze().numpy() == 1 for t in xs[ind]]\n",
    "    item = np.array(items)[item_vec][0]\n",
    "    relation = np.array(relations)[rel_vec][0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 15))\n",
    "    h1 = ax.barh(range(len(attributes)), net(xs[ind]).squeeze().numpy(),\n",
    "                 align='edge', height=0.4, tick_label=attributes)\n",
    "    h2 = ax.barh(range(len(attributes)), ys[ind].squeeze().numpy(),\n",
    "                 align='edge', height=-0.4, tick_label=attributes)\n",
    "    ax.legend([h1, h2], ['Actual', 'Expected'])\n",
    "    ax.set_title(f'{item} {relation}...', size='x-large')\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
